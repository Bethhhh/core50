---
layout: default
title: {{ site.name }}
---

<div id="home">

	<a name="intro"></a>
  <h1>Why CORe50?</h1>

One of the greatest goals of AI is building an artificial continuous learning agent which can construct a sophisticated understanding of the external world from its own experience through the <i>adaptive</i>, <i>goal-oriented</i> and <i>incremental</i> development of ever more complex skills and knowledge.<br><br>

Yet, <strong>Continuous/Lifelong learning (CL)</strong> of high-dimensional data streams is a challenging research problem far from being solved. In fact, fully retraining models each time new data becomes available is infeasible, due to computational and storage issues, while naïve continuous learning strategies have been shown to suffer from <strong>catastrophic forgetting</strong>. Moreover, even in the context of real-world object recognition applications (e.g. robotics), where continuous learning is crucial, very few datasets and benchmarks are available to evaluate and compare emerging techniques.<br><br>

In this page we provide a new dataset and benchmark <strong>CORe50</strong>, specifically designed for assessing <strong>C</strong>ontinuous Learning techniques in an <strong>O</strong>bject <strong>Re</strong>cognition context, along with a few baseline approaches for three different continuous learning scenarios.
If you plan to use this dataset or other resources you'll find in this page, <strong> please cite our latest paper</strong>: </p>
<div style="background-color: rgba(227, 246, 254, 0.79); padding: 20px; margin-top:20px;">
<div style="width:60px; height: 60px;float:left; padding-right:10px"><a href=""><img src="imgs/pdf_download.png" width="50px" style="padding:5px; vertical-align: middle;"></a></div>
<div style="margin-left:60px"> 
<i> Vincenzo Lomonaco and Davide Maltoni. <a href="http://proceedings.mlr.press/v78/lomonaco17a.html" target="_blank">"CORe50: a new Dataset and Benchmark for Continuous Object Recognition".</a> Proceedings of the 1st Annual Conference on Robot Learning, PMLR 78:17-26, 2017. </i>`</div>
</div>

	<a name="dataset"></a>
  <h1>Dataset</h1>
<p>
<strong>CORe50</strong>, specifically designed for (<strong>C</strong>)ontinuous (<strong>O</strong>)bject (<strong>Re</strong>)cognition, is a collection of 50 domestic objects belonging to 10 categories: plug adapters, mobile phones, scissors, light bulbs, cans, glasses, balls, markers, cups and remote controls. Classification can be performed at object level (50 classes) or at category level (10 classes). The first task (the default one) is much more challenging because objects of the same category are very difficult to be distinguished under certain poses.  <br><br>
The dataset has been collected in <strong>11 distinct sessions</strong> (8 indoor and 3 outdoor) characterized by different backgrounds and lighting. For each session and for each object, a 15 seconds video (at 20 fps) has been recorded with a Kinect 2.0 sensor delivering 300 RGB-D frames. <br><br>
Objects are hand hold by the operator and the camera <i>point-of-view</i> is that of the operator eyes. The operator is required to extend his arm and smoothly move/rotate the object in front of the camera. A subjective point-of-view with objects at grab-distance is well-suited for a number of robotic applications. The grabbing hand (left or right) changes throughout the sessions and relevant object occlusions are often produced by the hand itself.<br><br>
</p>
<div style="text-align: center">
<img src="imgs/classes.gif" alt="core50 dataset examples" style="width:80%; margin-top:10px;">
<p><strong>Fig.1</strong> <i>Example images of the 50 objects in CORe50. Each column denotes one of the 10 categories.</i> </p>
</div><br>
<p>
The presence of <strong>temporal coherent</strong> sessions (i.e., videos where the objects gently move in front of the camera) is another key feature since temporal smoothness can be used to simplify object detection, improve classification accuracy and to address <strong>semi-supervised</strong> (or <strong>unsupervised</strong>) scenarios.<br><br>

In <strong>Fig. 1</strong> you can see some image examples of the 50 objects in CORe50 where each column denotes one of the 10 categories and each row a different object. The full dataset consists of 164,866 128×128 RGB-D images: 11 sessions × 50 objects × (around 300) frames per session. Three of the eleven sessions (#3, #7 and #10) have been selected for test and the remaining 8 sessions are used for training. We tried to balance as much as possible the difficulty of training and test session with respect to: indoor/outdoor, holding hand (left or right) and complexity of the background. For more information about the dataset take a look a the section "<strong>CORe50</strong>" in the <a href="https://arxiv.org/abs/1705.03550" target="_blank">paper</a>.
</p>
	<a name="benchmark"></a>
  <h1>Benchmark</h1>
			Popular datasets such as <i>ImageNet</i> and <i>Pascal VOC</i>, provide a very good playground for classification and detection approaches. However, they have been designed with “static” evaluation protocols in mind; the entire dataset is split in just two parts: a training set is used for (one-shot) learning and a separate test set is used for accuracy evaluation. 

<br><br><strong>Splitting the training set into a number of batches is essential</strong> to train and test continuous learning approaches, a hot research topic that is currently receiving much attention. Unfortunately, most of the existing datasets are not well suited to this purpose because they lack a fundamental ingredient: the presence of multiple (unconstrained) views of the same objects taken in different sessions (varying background, lighting, pose, occlusions, etc.). 
			Focusing on Object Recognition we consider three continuous learning scenarios:<br><br>
<ul class="posts">
			<li><span><strong>New Instances (NI)</strong>: new training patterns of the same classes becomes available in subsequent batches with new poses and environment conditions (illuminations, background, occlusions, etc..). A good model is expected to incrementally consolidate its knowledge about the known classes without compromising what it learned before.</li><br>
			<li><span><strong>New Classes (NC)</strong>: new training patterns belonging to different classes becomes available in subsequent batches. In this case the model should be able to deal with the new classes without losing accuracy on the previous ones. </li><br>
			<li><span><strong>New Instances and Classes (NIC)</strong>: new training patterns belonging both to known and new classes becomes available in subsequent training batches. A good model is expected to consolidate its knowledge about the known classes and to learn the new ones.</li><br>
</ul>

<div style="text-align: center">
<img src="imgs/NI_caffenet.svg" alt="core50 dataset examples" style="width:49%; margin-top:10px;">
<img src="imgs/NC_caffenet.svg" alt="core50 dataset examples" style="width:49%; margin-top:10px;">
<img src="imgs/NIC_caffenet.svg" alt="core50 dataset examples" style="width:50%; margin-top:10px;">
<p><strong>Fig. 2</strong> <i>Mid-CaffeNet accuracy in the NI, NC and NIC scenarios (average over 10 runs).</i> </p>
</div><br>

As argued by many researchers, naïve approaches cannot avoid catastrophic forgetting in complex real-world scenarios such as <strong>NC</strong> and <strong>NIC</strong>. In our work we have designed <strong>simple baselines</strong> which can perform markedly better than naïve strategies but still leave much room for improvements (see <strong> Fig. 2</strong>). Check it out the full results on our <a href="https://arxiv.org/abs/1705.03550" target="_blank">paper</a> or download them as tsv or python dicts in the section below!

<h3> Differences with other common CL Benchmarks </h3>
<p>
As you probably noted from the description above, our benchmark is somehow different from the common benchmarks used in the Continuous Learning literature where the focus is mostly about learning a (short) sequence of tasks:

<ol class="posts">
      <li><span><a href="https://arxiv.org/pdf/1612.00796.pdf" target="_blank">Permuted MNIST</a></li>
      <li><span><a href="https://arxiv.org/pdf/1703.04200.pdf" target="_blank">MNIST Split</a></li>
      <li><span><a href="https://arxiv.org/pdf/1703.04200.pdf" target="_blank">CIFAR10/100 Split</a></li>
      <li><span><a href="https://arxiv.org/pdf/1612.00796.pdf" target="_blank">Atari Games</a></li>
</ol>

In the first and last benchmarks the output distribution stay fixed across all the tasks (each task has all the possible MNIST digits and actions in the Atari games) making it more similar to our <strong>NI scenario</strong>. <br>
However, in the other cases, the output distributions are always <strong>treated separately</strong> for each task (each one has its own separated classes) making it more easier than our <strong>NC scenario</strong> where we are actually <strong>adding new dimensions</strong> to the same output distribution (new classes which have to be distinguished from <strong>all</strong> the previous ones).<br><br>

This is not only more suitable for <i>Object Recognition</i> (where we want to recognize an object across the whole objects space) but for any real-world application where the notion of <i>"task"</i> is futile if not misleading. Moreover, it allow us to introduce the <strong>NIC scenario</strong> on which no CL strategy has ever been assessed before.<br><br>

Please also note that we decided for simplicity to <strong>keep the test set fixed</strong> across all batches (check section 5.2 of the <a href="https://arxiv.org/abs/1705.03550" target="_blank">paper</a> for further discussions about this choice).
</p>

	<a name="download"></a>
  <h1>Download</h1>
	In order to facilitate the usage of the benchmark we freely release the dataset, the code to reproduce the baselines and all the materials which could be useful for speeding-up the creation of new <i>continuous learning</i> strategies on CORe50. <br>

<h3> Dataset </h3>
<p>The dataset directory tree is not that different from what you may expect. For each session (<i>s1, s2, ..., s11</i>) we have 50 directories (<i>o1, o2, ..., o50</i>) representing the 50 objects contained in the dataset. Below you can see to which class each object instance id corresponds to:
<pre><code>[o1, ..., o5] -> plug adapters
[o6, ..., o10] -> mobile phones
[o11, ..., o15] -> scissors
[o16, ..., o20] -> light bulbs
[o21, ..., o25] -> cans
[o26, ..., o30] -> glasses
[o31, ..., o35] -> balls
[o36, ..., o40] -> markers
[o41, ..., o45] -> cups
[o46, ..., o50] -> remote controls
</code></pre></p>

<div style="background-color: rgba(227, 246, 254, 0.79); padding: 20px; margin-top:20px;">
<div style="width:60px; height: 60px;float:left; padding-right:10px"><a href="data/core50_class_names.txt"><img src="imgs/download.ico" width="50px" style="padding:5px; vertical-align: middle;"></a></div>
<div style="margin-top:10px; margin-bottom:10px;margin-left:60px"> 
<strong>core50_class_names.txt</strong>. <i> Ordered list of CORe50 class names. </i></div>
</div>

<p>In each object directories, the temporal coherent frames are characterized by an unique filename with the format <i>"C_[session_num]_[obj_num]_[frame_seq_id].png"</i> :</p>
<pre><code>CORe50/
  |
  |--- s1/
  |    |------ o1/
  |    |       |---- C_01_01_XXX.png
  |    |       |---- ...
  |    | 
  |    |------ o2/
  |    |------ ...
  |    |------ o50/
  |
  |--- s2/
  |--- s3/
  |--- ...
  |--- s11/
</code></pre>

<p> Since we make available both the 350 x 350 original images and their cropped version (128 x 128), we thought it would be useful also to release the bounding boxes with respect to the original image size. <br>
The bbox coordinates for each image are automatically extracted based on a very simple tracking technique, briefly described in the <a href="https://128.84.21.199/pdf/1705.03550">paper</a>. In the <i>bbox.zip</i> you can download below you will find for each object and session a different txt file. Each file follows the format:</p>
<pre><code>Color000: 142 160 269 287
Color001: 143 160 270 287
Color002: 145 160 272 287
Color003: 149 160 276 287
Color004: 149 159 276 286
...
</code></pre>

<p>So, for each image <i>ColorID</i>, we have the bbox in the common format [<i>min x, min y, max x, max y</i>] of the <a href="https://it.mathworks.com/help/images/image-coordinate-systems.html">image coordinates system</a>.</p>

<div style="background-color: rgba(227, 246, 254, 0.79); padding: 20px; margin-top:20px;">
<div style="width:60px; height: 60px;float:left; padding-right:10px"><a href="http://bias.csr.unibo.it/maltoni/download/core50/core50_350x350.zip"><img src="imgs/download.ico" width="50px" style="padding:5px; vertical-align: middle;"></a></div>
<div style="margin-top:10px; margin-bottom:10px;margin-left:60px"> 
<strong>full-size_350x350_images.zip</strong>. <i> 350 x 350 images before tracking in the png format.</i></div>
</div>

<div style="background-color: rgba(227, 246, 254, 0.79); padding: 20px; margin-top:10px;">
<div style="width:60px; height: 60px;float:left; padding-right:10px"><a href="http://bias.csr.unibo.it/maltoni/download/core50/core50_128x128.zip"><img src="imgs/download.ico" width="50px" style="padding:5px; vertical-align: middle;"></a></div>
<div style="margin-top:10px; margin-bottom:10px;margin-left:60px"> 
<strong>cropped_128x128_images.zip</strong>. <i> 128 x 128 images used for the CORe50 benchmark in the png format.</i></div>
</div>

<div style="background-color: rgba(227, 246, 254, 0.79); padding: 20px; margin-top:10px;">
<div style="width:60px; height: 60px;float:left; padding-right:10px"><a href="data/bbox.zip"><img src="imgs/download.ico" width="50px" style="padding:5px; vertical-align: middle;"></a></div>
<div style="margin-top:10px; margin-bottom:10px;margin-left:60px"> 
<strong>bbox.zip</strong>. <i>Bounding boxes for the full-size version in the txt format.</i></div>
</div>

<p>If you are a Python user you can also benefit by our npz version of the dataset <i>core50_imgs.npz</i> accompanied by the <i>paths.pkl</i> file which contains the path corresponding to each image.</p>

			<div class="highlight highlight-Python" style="margin-top:15px">
			<pre><span class="pl-c"> # loading the npz and picked file</span><br> >>><span class="pl-k"> import</span> numpy <span class="pl-k">as</span> np <br> >>><span class="pl-k"> import</span> pickle <span class="pl-k">as</span> pkl <br> >>> pkl_file <span class="pl-k">= open</span>(<span class="pl-s">'paths.pkl'</span>, <span class="pl-s">'rb'</span>) <br> >>> paths <span class="pl-k">=</span> pkl.load(pkl_file) <br> >>> imgs <span class="pl-k">=</span> np.load(<span class="pl-s">'core50_imgs.npz'</span>)['x'] <br><br><span class="pl-c"> # Files dimensions</span><br> >>><span class="pl-k"> print </span>imgs.shape <br> (164866, 128, 128, 3)<br> >>><span class="pl-k"> print </span>len(paths)<br> 164866 </pre>
			</div>

<div style="background-color: rgba(227, 246, 254, 0.79); padding: 20px; margin-top:10px;">
<div style="width:60px; height: 60px;float:left; padding-right:10px"><a href="http://bias.csr.unibo.it/maltoni/download/core50/core50_imgs.npz"><img src="imgs/download.ico" width="50px" style="padding:5px; vertical-align: middle;"></a></div>
<div style="margin-top:10px; margin-bottom:10px;margin-left:60px"> 
<strong>core50_imgs.npz</strong>. <i> Np.uint8 PIL images of the entire Cropped_128x128 version of the dataset.</i></div>
</div>

<div style="background-color: rgba(227, 246, 254, 0.79); padding: 20px; margin-top:10px;">
<div style="width:60px; height: 60px;float:left; padding-right:10px"><a href="data/paths.pkl"><img src="imgs/download.ico" width="50px" style="padding:5px; vertical-align: middle;"></a></div>
<div style="margin-top:10px; margin-bottom:10px;margin-left:60px"> 
<strong>paths.pkl</strong>. <i> Python list of Strings containing the corresponding patch of each image in the core50_imgs.npz file.</i></div>
</div>


<p>In order to better track the moving objects or to further improve the object recognition accuracy, we release also the <strong>depth map</strong> in the same format we have seen before for the colored images:</p>

<pre><code>CORe50/
  |
  |--- s1/
  |    |------ o1/
  |    |       |---- D_01_01_XXX.png
  |    |       |---- ...
  |    | 
  |    |------ o2/
  |    |------ ...
  |    |------ o50/
  |
  |--- s2/
  |--- s3/
  |--- ...
  |--- s11/
</code></pre>

<p>As you can see from <strong>Fig. 3</strong>, the depth map is not perfect (further enhancing preprocessing steps can be performed) but it can be also easily converted in a <strong>segmentation map</strong> using a moving threshold.</p>

<div style="text-align: center">
<img src="imgs/D_05_33_179.png" alt="core50 dataset examples" style="width:30%; margin-top:10px; margin-right:20px;    background-image: linear-gradient(45deg, #808080 25%, transparent 25%), linear-gradient(-45deg, #808080 25%, transparent 25%), linear-gradient(45deg, transparent 75%, #808080 75%), linear-gradient(-45deg, transparent 75%, #808080 75%);
  background-size: 20px 20px;
  background-position: 0 0, 0 10px, 10px -10px, -10px 0px;">
<img src="imgs/C_05_33_179.png" alt="core50 dataset examples" style="width:30%; margin-top:10px;">
<p><strong>Fig. 3</strong> <i>Example of a depth map (dark is far) for the object 33 with a complex background. Chessboard pattern where the depth information is missing.</i> </p>
</div><br>

<div style="background-color: rgba(227, 246, 254, 0.79); padding: 20px; margin-top:10px;">
<div style="width:60px; height: 60px;float:left; padding-right:10px"><a href="http://bias.csr.unibo.it/maltoni/download/core50/core50_350x350_depth.zip"><img src="imgs/download.ico" width="50px" style="padding:5px; vertical-align: middle;"></a></div>
<div style="margin-top:10px; margin-bottom:10px;margin-left:60px"> 
<strong>full-size_350x350_depth.zip</strong>. <i>Depth map for the full-size version in the png format (grayscale, transparent when the depth information is missing).</i></div>
</div>

<div style="background-color: rgba(227, 246, 254, 0.79); padding: 20px; margin-top:10px;">
<div style="width:60px; height: 60px;float:left; padding-right:10px"><a href="http://bias.csr.unibo.it/maltoni/download/core50/core50_128x128_depth.zip""><img src="imgs/download.ico" width="50px" style="padding:5px; vertical-align: middle;"></a></div>
<div style="margin-top:10px; margin-bottom:10px;margin-left:60px"> 
<strong>cropped_128x128_depth.zip</strong>. <i>Depth map for the cropped version in the png format (grayscale, transparent when the depth information is missing).</i></div>
</div>

<p>Finally, to make sure everything is there, you can check the exact number of (color + depth) images you're going to find for each object and session: </p>

<div style="background-color: rgba(227, 246, 254, 0.79); padding: 20px; margin-top:20px;">
<div style="width:60px; height: 60px;float:left; padding-right:10px"><a href="data/dataset_dims.zip"><img src="imgs/download.ico" width="50px" style="padding:5px; vertical-align: middle;"></a></div>
<div style="margin-top:10px; margin-bottom:10px;margin-left:60px"> 
<strong>dataset_dims.zip</strong>. <i> Exact number of frames for each object and session.</i></div>
</div>

<h3> Configuration Files </h3>
<p>Do you wanna use a different DL framework or programming language but still being able to compare your results with our benchmark? Well that's easy! Just <strong>download the batches filelists for each experiment</strong> in plain .txt format!
The filelists directory tree will look like this:</p>
<pre><code>filelists/
  |
  |--- NI_inc/
  |       |------ Run0/
  |       |         |------ train_batch_00_filelist.txt
  |       |         |------ train_batch_01_filelist.txt
  |       |         |------ ...
  |       |         |------ test_filelist.txt
  |       |         
  |       |------ Run1/
  |       |------ ...
  |       |------ Run9/
  |
  |--- NI_cum/
  |--- NC_inc/
  |--- NC_cum/
  |--- NIC_inc/
  |--- NIC_cum/
</code></pre>

So, for each scenario (NI, NC, NIC) we have two different filelists folders depending on the main strategy, cumulative or incremental, with the <i>_cum</i> or <i>_inc</i> suffix respectively. Then, for each of these folders, the configuration files of 10 different runs are reported. For each of these runs a number of <i>"train_batch_XX_filelist.txt"</i> is available along with the <i>"test_filelist.txt"</i>. Each file is formatted in the <a href="https://ceciliavision.wordpress.com/2016/03/08/caffedata-layer/">caffe filelist format</a>. <br><br>

<strong>PLEASE NOTE</strong> that:

<ol class="posts" style='margin-top: 10px'>
      <li></strong>The order <strong> may highly impact</strong> the accuracy in a continuous learning scenario! This is why a multiple runs configuration is needed;</li>
      <li><strong>For the NC scenario the mappings "object -> label" are different for each run </strong>. That's because some CL strategies prefer labels to be continuous for each incremental batch (i.e. if we have 3 objects we want the labels to be exactly 0, 1 and 2)!</li>
</ol>

Recovering the mapping from the filelists is straightforward but a bit tedious so we have already prepared a Python dict for you (download link below) such that you access the mapping as easy as:

			<div class="highlight highlight-Python" style="margin-top:15px">
			<pre><span class="pl-c"> # loading the picked file</span><br> >>><span class="pl-k"> import</span> pickle <span class="pl-k">as</span> pkl <br> >>> pkl_file <span class="pl-k">= open</span>(<span class="pl-s">'labels2names.pkl'</span>, <span class="pl-s">'rb'</span>) <br> >>> labels2names <span class="pl-k">=</span> pkl.load(pkl_file)<br><br><span class="pl-c"> # using the dict like labels2names[scenario][run]</span><br> >>><span class="pl-k"> print </span>exps[<span class="pl-s">'ni'</span>][0] <br> {0: 'plug_adapter1', 1: 'plug_adapter2', ... }
			</div>

<div style="background-color: rgba(227, 246, 254, 0.79); padding: 20px; margin-top:20px;">
<div style="width:60px; height: 60px;float:left; padding-right:10px"><a href="data/batches_filelists.zip"><img src="imgs/download.ico" width="50px" style="padding:5px; vertical-align: middle;"></a></div>
<div style="margin-top:10px; margin-bottom:10px;margin-left:60px"> 
<strong>batches_filelists.zip</strong>. <i>Plain text filelists for each experiment and run</i>.</div>
</div>

<div style="background-color: rgba(227, 246, 254, 0.79); padding: 20px; margin-top:20px;">
<div style="width:60px; height: 60px;float:left; padding-right:10px"><a href="data/labels2names.pkl"><img src="imgs/download.ico" width="50px" style="padding:5px; vertical-align: middle;"></a></div>
<div style="margin-top:10px; margin-bottom:10px;margin-left:60px"> 
<strong>labels2names.pkl</strong>. <i>Python dict with the "label->object" mapping for each scenario</i>.</div>
</div>

<h3> Results </h3>
If you want to compare your strategy with our benchmark or just take a closer look at the tabular results, we are making available the <strong>tab separated values (.tsv)</strong> files for each experiment! Which you can easily import in excel or parse with your favourite programming language ;-) The format of the text files is the following:
<pre style="margin-top:15px"><code>###################################
# scenario: NI
# net: mid-caffenet
# strategy: naive
###################################
RunID	Batch0	Batch1	Batch2	Batch3	Batch4	Batch5	Batch6	Batch7
0	44,30%	35,50%	55,26%	55,86%	54,39%	48,90%	47,77%	60,71%
1	47,56%	48,94%	51,83%	53,64%	51,26%	43,39%	58,98%	55,61%
2	40,06%	53,04%	46,65%	45,19%	44,09%	50,92%	54,42%	57,01%
3	46,29%	48,66%	54,64%	52,48%	44,36%	51,55%	50,64%	44,26%
4	28,40%	43,77%	43,27%	51,00%	58,26%	56,89%	52,08%	61,94%
5	34,65%	34,36%	50,14%	56,89%	59,67%	56,66%	56,16%	52,66%
6	40,80%	51,92%	51,44%	53,12%	58,19%	49,72%	50,33%	48,14%
7	30,96%	51,54%	46,28%	51,69%	56,88%	55,44%	55,72%	49,98%
8	34,32%	32,19%	40,55%	52,06%	57,86%	57,41%	57,80%	61,86%
9	46,06%	47,62%	51,80%	46,92%	42,30%	57,70%	59,98%	50,01%
avg	38,59%	44,44%	48,90%	52,44%	53,88%	52,32%	53,77%	54,69%
dev.std	6,87%	7,88%	4,84%	3,59%	6,75%	4,74%	4,06%	6,18%
....
</code></pre>

For convenience, we have split the .tsv in three main files with respect of the three different scenarios, but with this format you can easily concatenate them in a single file if you need it.

For an even simpler analysis we have already prepared for you a <strong>Python pickled OrderedDict</strong> which you can simply load and use to access/plot the results:
			<div class="highlight highlight-Python" style="margin-top:15px">
			<pre><span class="pl-c"> # loading the picked file</span><br> >>><span class="pl-k"> import</span> pickle <span class="pl-k">as</span> pkl <br> >>> pkl_file <span class="pl-k">= open</span>(<span class="pl-s">'results.pkl'</span>, <span class="pl-s">'rb'</span>) <br> >>> exps <span class="pl-k">=</span> pkl.load(pkl_file)<br><br><span class="pl-c"> # using the dict like exps[scenario][net][strategy][run][batch]</span><br> >>><span class="pl-k"> print </span>exps[<span class="pl-s">'NI'</span>][<span class="pl-s">'mid-caffenet'</span>][<span class="pl-s">'naive'</span>][<span class="pl-s">'avg'</span>].values() <br> [38.59, 44.44, 48.90, 52.44, 53.88, 52.32, 53.77, 54.69]<br> >>><span class="pl-k"> print </span>exps[<span class="pl-s">'NI'</span>][<span class="pl-s">'mid-caffenet'</span>][<span class="pl-s">'naive'</span>][<span class="pl-s">'avg'</span>][<span class="pl-s">'Batch0'</span>] <br> 38.59</pre>
			</div>

A similar approach can be used with the <i>sequential experiments</i> of <strong>Fig. 5</strong> in the paper. You can find more code examples about extracting results data in the <a href="https://github.com/vlomonaco/core50">README.txt</a> of the <a href="https://github.com/vlomonaco/core50">Github repo</a>. Otherwise you can directy download the tsvs and the python dicts here:

<div style="background-color: rgba(227, 246, 254, 0.79); padding: 20px; margin-top:20px;">
<div style="width:60px; height: 60px;float:left; padding-right:10px"><a href="data/results_tsv.zip"><img src="imgs/download.ico" width="50px" style="padding:5px; vertical-align: middle;"></a></div>
<div style="margin-top:10px; margin-bottom:10px;margin-left:60px"> 
<strong>tsv_results.zip</strong>. <i> Tab-separated-values results for each scenario</i>.</div>
</div>

<div style="background-color: rgba(227, 246, 254, 0.79); padding: 20px; margin-top:10px;">
<div style="width:60px; height: 60px;float:left; padding-right:10px"><a href="data/results.pkl"><img src="imgs/download.ico" width="50px" style="padding:5px; vertical-align: middle;"></a></div>
<div style="margin-top:10px; margin-bottom:10px;margin-left:60px"> 
<strong>results.pkl</strong>. <i>Pre-loaded Python dict (pickled)</i>.</div>
</div>
<div style="background-color: rgba(227, 246, 254, 0.79); padding: 20px; margin-top:10px;">
<div style="width:60px; height: 60px;float:left; padding-right:10px"><a href="data/seq.tsv"><img src="imgs/download.ico" width="50px" style="padding:5px; vertical-align: middle;"></a></div>
<div style="margin-top:10px; margin-bottom:10px;margin-left:60px"> 
<strong>seq.tsv</strong>. <i> Tab-separated-values results for the sequential experiments</i>.</div>
</div>

<div style="background-color: rgba(227, 246, 254, 0.79); padding: 20px; margin-top:10px;">
<div style="width:60px; height: 60px;float:left; padding-right:10px"><a href="data/seq_results.pkl"><img src="imgs/download.ico" width="50px" style="padding:5px; vertical-align: middle;"></a></div>
<div style="margin-top:10px; margin-bottom:10px;margin-left:60px"> 
<strong>seq_results.pkl</strong>. <i>Pre-loaded Python dict (pickled) for the sequential experiments</i>.</div>
</div>

<h3> Code and Data Loaders </h3>
The Python code for reproducing the experiments in the paper is already available in the master branch of <a href="https://github.com/vlomonaco/core50"> this github repository</a>!<br><br>

As you might have noticed, assessing your own CL strategies on CORe50 wouldn't be as simple as loading the npz file since you have to deal with a number of runs and configurations. That's way we also provide a <i>super fast</i> and <i>flexible</i> <strong>Python Data Loader</strong> which can handle that for you. For example: 

			<div class="highlight highlight-Python" style="margin-top:15px">
			<pre><span class="pl-c"> # importing CORe50</span><br> >>><span class="pl-k"> import</span> data_loader <br> >>> train_set <span class="pl-k">=</span> data_loader.CORE50(root<span class="pl-k">=</span><span class="pl-s">'/home/admin/core50_128x128'</span>, cumul<span class="pl-k">=False</span>, run<span class="pl-k">=</span>0)<br><br><span class="pl-c"> # Using the data loader</span><br> >>><span class="pl-k"> for </span>batch<span class="pl-k"> in</span> train_set: <br><span class="pl-c"> ...     # This is the training batch not the mini-batch</span> <br> ...     x, y <span class="pl-k">= </span> batch <br></pre>
			</div><br>

Take a look at the Python class <a href='https://github.com/vlomonaco/core50/blob/master/scripts/python/data_loader.py'>here</a> for more details. As you can see we thought at two modalities. The first one for small RAM devices where we load the batch imgs <i>on-the-fly</i> as they are needed and the second one in which we preload the entire dataset and we use a <i>Look-UP table</i> to figure out which subset of data we need.<br><br>

if you are a PyTorch user, we have just implemented the <strong>Pytorch Data Loader</strong> (pending <a href='https://github.com/pytorch/vision/pull/340'>Pull Request</a>), you can download <a href='https://github.com/pytorch/vision/pull/340'>here</a> but up to now it has only the "loading on-the-fly" (even if multi-threads) modality.<br><br>

If you want to contribute or you find a bug, please <strong>make a PR</strong> or simply <strong>open an issue</strong> (also questions are welcomed)! We guarantee at least an answer in 48hrs! :-)

<div style="background-color: rgba(227, 246, 254, 0.79); padding: 20px; margin-top:20px;">
<div style="width:60px; height: 60px;float:left; padding-right:10px"><a href="data/labels.pkl"><img src="imgs/download.ico" width="50px" style="padding:5px; vertical-align: middle;"></a></div>
<div style="margin-top:10px; margin-bottom:10px;margin-left:60px"> 
<strong>labels.pkl</strong>. <i>Pre-loaded Python dict (pickled) for the labels of each run</i>.</div>
</div>

<div style="background-color: rgba(227, 246, 254, 0.79); padding: 20px; margin-top:10px;">
<div style="width:60px; height: 60px;float:left; padding-right:10px"><a href="data/LUP.pkl"><img src="imgs/download.ico" width="50px" style="padding:5px; vertical-align: middle;"></a></div>
<div style="margin-top:10px; margin-bottom:10px;margin-left:60px"> 
<strong>LUP.pkl</strong>. <i> Look-UP Table with the patterns order for each run</i>.</div>
</div>

<div style="background-color: rgba(227, 246, 254, 0.79); padding: 20px; margin-top:10px;">
<div style="width:60px; height: 60px;float:left; padding-right:10px"><a href="https://github.com/vlomonaco/core50/archive/master.zip"><img src="imgs/download.ico" width="50px" style="padding:5px; vertical-align: middle;"></a></div>
<div style="margin-top:10px; margin-bottom:10px; margin-left:60px"> 
<strong>core50-master.zip</strong>. <i> All the code, materials, scripts in order to use/reproduce the benchmark.</i></div>
</div>

	<a name="contacts"></a>
  <h1>Contacts</h1>
	This Dataset and Benchmark has been developed at the University of Bologna with the effort of different people:<br><br>
  <ul class="posts">
     <li><span><a href="http://vincenzolomonaco.com"> Vincenzo Lomonaco</a> &#8226; <i>PhD Student</i></li>
 		<li><span><a href="http://bias.csr.unibo.it/maltoni/main.htm"> Davide Maltoni</a> &#8226; <i>Professor</i></li>
 		<li><span><a href="https://www.linkedin.com/in/riccardo-monica-2577a1123/"> Riccardo Monica </a> &#8226; <i>Master Student</i></li>
		<li><span><a href="https://www.linkedin.com/in/martin-cimmino-23a87086/"> Martin Cimmino</a> &#8226; <i>Master Student</i></li>
  </ul>

	<br>For further inquiries you are free to contact <strong>Vincenzo Lomonaco</strong> through his email: <i>vincenzo.lomonaco@unibo.it</i>

